{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK6q5Jyy1FKY"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOGOj4NV0Vq6"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "# Choose your models - draft should be much smaller than target\n",
        "# Using Mistral-7B-Instruct-v0.2 as a larger, publicly accessible target model\n",
        "target_model_name = \"google/gemma-7b-it\"\n",
        "# Using TinyLlama-1.1B-Chat-v1.0 as a smaller, publicly accessible draft model\n",
        "draft_model_name = \"google/gemma-2b-it\"\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load tokenizer (must be the same for both models)\n",
        "tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
        "\n",
        "# Load target model (the large, high-quality model)\n",
        "print(\"Loading target model...\")\n",
        "target_model = AutoModelForCausalLM.from_pretrained(\n",
        "    target_model_name,\n",
        "    dtype=torch.float16,  # Use fp16 for faster inference\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load draft model (the small, fast model)\n",
        "print(\"Loading draft model...\")\n",
        "draft_model = AutoModelForCausalLM.from_pretrained(\n",
        "    draft_model_name,\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"Models loaded successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScAUKOR60k-8"
      },
      "source": [
        "To access gated models like Gemma, you need to log in to Hugging Face.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Get a Hugging Face API Token:** Go to [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) and create a new access token (ensure it has at least 'read' permissions).\n",
        "2.  **Log in:**\n",
        "    *   **Option 1 (Recommended in Colab):** Run the following code in a new cell and paste your token when prompted:\n",
        "\n",
        "        ```python\n",
        "        from huggingface_hub import login\n",
        "        login()\n",
        "        ```\n",
        "\n",
        "    *   **Option 2 (Environment Variable):** Set the `HF_TOKEN` environment variable before running any code that accesses Hugging Face. For example:\n",
        "\n",
        "        ```python\n",
        "        import os\n",
        "        os.environ[\"HF_TOKEN\"] = \"hf_YOUR_TOKEN_HERE\"\n",
        "        ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss64KU3D8SSl"
      },
      "outputs": [],
      "source": [
        "# Create a prompt\n",
        "prompt = \"Quantum entanglement is a phenomenon where\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "print(f\"Input prompt: {prompt}\")\n",
        "print(f\"Input token count: {inputs['input_ids'].shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rpgfkyr9nc1K"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Standard generation (no speculation)\n",
        "print(\"\\n--- Standard Generation (Baseline) ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "baseline_output = target_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=50,\n",
        "    # do_sample=False,  # Greedy decoding\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "baseline_time = time.time() - start_time\n",
        "baseline_text = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Generated text:\\n{baseline_text}\\n\")\n",
        "print(f\"Time taken: {baseline_time:.2f} seconds\")\n",
        "print(f\"Tokens per second: {50/baseline_time:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JLGccrbPnhGG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import warnings # Import warnings module\n",
        "\n",
        "\n",
        "# Speculative decoding - just add assistant_model parameter!\n",
        "print(\"\\n--- Speculative Decoding ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\") # Ignore all warnings within this block\n",
        "    speculative_output = target_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True,  # set to False for greedy decoding\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        assistant_model=draft_model,   # This enables speculative decoding!\n",
        "        num_assistant_tokens = 10\n",
        "    )\n",
        "\n",
        "speculative_time = time.time() - start_time\n",
        "speculative_text = tokenizer.decode(speculative_output[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Generated text:\\n{speculative_text}\\n\")\n",
        "print(f\"Time taken: {speculative_time:.2f} seconds\")\n",
        "print(f\"Tokens per second: {50/speculative_time:.2f}\")\n",
        "\n",
        "# Calculate speedup\n",
        "speedup = baseline_time / speculative_time\n",
        "print(f\"\\n Speedup: {speedup:.2f}x faster!\")"
      ]
    }
  ]
}