{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install PySpark"
      ],
      "metadata": {
        "id": "EnkdF6a8IJNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40wRo96rr55a",
        "outputId": "77638389-c474-44c0-98e4-68932fa52e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=6bd80e2df67a29c669daab45ed1eb501ce4a7f36d432bceb1d510132c890bcd0\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "! pip3 install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Start a PySpark Session"
      ],
      "metadata": {
        "id": "s5NnsPriK2WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "\t.appName(\"DataCleaning\") \\\n",
        "\t.getOrCreate()\n"
      ],
      "metadata": {
        "id": "sjLGh31-zcjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generate a Sample Dataset"
      ],
      "metadata": {
        "id": "XQmyi3t9K-vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Function to generate random data with some missing values and duplicates\n",
        "def generate_data(n):\n",
        "    customer_ids = [f'C{str(i).zfill(5)}' for i in range(1, 101)]\n",
        "    product_categories = ['Electronics', 'Books', 'Clothing', 'Groceries', 'Furniture']\n",
        "\n",
        "    data = []\n",
        "    for i in range(n):\n",
        "        customer_id = random.choice(customer_ids) if i % 10 != 0 else None  # Introduce some missing values\n",
        "        transaction_id = f'T{str(random.randint(10000, 99999))}'\n",
        "        transaction_date = pd.Timestamp('2023-01-01') + pd.to_timedelta(random.randint(0, 180), unit='d')\n",
        "        amount = round(random.uniform(5, 500), 2)\n",
        "        product_category = random.choice(product_categories)\n",
        "        data.append((customer_id, transaction_id, transaction_date, amount, product_category))\n",
        "\n",
        "        # Introduce duplicates\n",
        "        data.extend(data[:10])\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "7mZDk8Arzlhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 10,000 rows of data\n",
        "data = generate_data(10_000)\n",
        "\n",
        "# Convert to a Pandas DataFrame and then to PySpark DataFrame\n",
        "columns = ['CustomerID', 'TransactionID', 'TransactionDate', 'Amount', 'ProductCategory']\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "spark_df = spark.createDataFrame(df)\n",
        "\n",
        "spark_df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laHfRBDkzp1z",
        "outputId": "137cbd86-ede9-4cc5-bf4b-a753e12bfb4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-------------------+------+---------------+\n",
            "|CustomerID|TransactionID|    TransactionDate|Amount|ProductCategory|\n",
            "+----------+-------------+-------------------+------+---------------+\n",
            "|      NULL|       T17203|2023-03-20 00:00:00|221.92|          Books|\n",
            "|      NULL|       T17203|2023-03-20 00:00:00|221.92|          Books|\n",
            "|    C00058|       T63296|2023-02-11 00:00:00|157.92|      Groceries|\n",
            "|      NULL|       T17203|2023-03-20 00:00:00|221.92|          Books|\n",
            "|      NULL|       T17203|2023-03-20 00:00:00|221.92|          Books|\n",
            "+----------+-------------+-------------------+------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz3u7w8R8eQF",
        "outputId": "181b8c62-3d8f-4585-ed60-1c2adb33b8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('CustomerID', 'string'),\n",
              " ('TransactionID', 'string'),\n",
              " ('TransactionDate', 'date'),\n",
              " ('Amount', 'double'),\n",
              " ('ProductCategory', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Handle Missing Values"
      ],
      "metadata": {
        "id": "nrouSi23Dw_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing CustomerID with a default value\n",
        "spark_df = spark_df.fillna({\"CustomerID\": \"Unknown\"})\n"
      ],
      "metadata": {
        "id": "ku6AdAO6z9PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Remove Duplicates"
      ],
      "metadata": {
        "id": "tuP2lpcYD6Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, min, max\n",
        "\n",
        "# Normalize the 'Amount' column\n",
        "min_amount = spark_df.agg(min(col(\"Amount\"))).collect()[0][0]\n",
        "max_amount = spark_df.agg(max(col(\"Amount\"))).collect()[0][0]\n",
        "\n",
        "spark_df = spark_df.withColumn(\"Amount\", (col(\"Amount\") - min_amount) / (max_amount - min_amount))\n"
      ],
      "metadata": {
        "id": "eomZcsnW0HCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Transform Columns"
      ],
      "metadata": {
        "id": "nzJzlkajIzkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, min, max\n",
        "\n",
        "# Normalize the 'Amount' column\n",
        "min_amount = spark_df.agg(min(col(\"Amount\"))).collect()[0][0]\n",
        "max_amount = spark_df.agg(max(col(\"Amount\"))).collect()[0][0]\n",
        "\n",
        "spark_df = spark_df.withColumn(\"Amount\", (col(\"Amount\") - min_amount) / (max_amount - min_amount))"
      ],
      "metadata": {
        "id": "5TPaJLV3I1r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Handle Outliers"
      ],
      "metadata": {
        "id": "y0JtNxWVJCju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, expr\n",
        "\n",
        "# Calculate Q1, Q3, and IQR\n",
        "quantiles = spark_df.approxQuantile(\"Amount\", [0.25, 0.75], 0.05)\n",
        "Q1 = quantiles[0]\n",
        "Q3 = quantiles[1]\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the upper and lower bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Filter out the outliers\n",
        "spark_df = spark_df.filter((col(\"Amount\") >= lower_bound) & (col(\"Amount\") <= upper_bound))"
      ],
      "metadata": {
        "id": "KsPQQuGrJHYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 7. Convert Data Types"
      ],
      "metadata": {
        "id": "8mpXH0cdJL8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Convert 'TransactionDate' to date format\n",
        "# (not quite needed for this dataset)\n",
        "spark_df = spark_df.withColumn(\"TransactionDate\", to_date(col(\"TransactionDate\")))\n"
      ],
      "metadata": {
        "id": "46QEuADu0LIS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}