{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install PySpark"
      ],
      "metadata": {
        "id": "5oNZgpo8Ljzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Se5rOKKjzk",
        "outputId": "223f9be2-a51e-4006-e89d-b077b3d546c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=bbfd80a589ea8e2302f3938fd11b4434a84633b28244a0229ecf62245ae601d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Start a PySpark Session\n"
      ],
      "metadata": {
        "id": "myuR4RIqLmvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "caz_ZJqBKXU2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "\t.appName(\"ReadCSV\") \\\n",
        "\t.getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generate a Sample CSV File"
      ],
      "metadata": {
        "id": "zv3SAft7Ltuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Function to generate random transaction data\n",
        "def generate_data(n):\n",
        "    customer_ids = [f'C{str(i).zfill(5)}' for i in range(1, 101)]\n",
        "    product_categories = ['Electronics', 'Books', 'Clothing', 'Groceries', 'Furniture']\n",
        "\n",
        "    data = []\n",
        "    for _ in range(n):\n",
        "        customer_id = random.choice(customer_ids)\n",
        "        transaction_id = f'T{str(random.randint(10000, 99999))}'\n",
        "        transaction_date = pd.Timestamp('2023-01-01') + pd.to_timedelta(random.randint(0, 180), unit='d')\n",
        "        amount = round(random.uniform(5, 500), 2)\n",
        "        product_category = random.choice(product_categories)\n",
        "        data.append((customer_id, transaction_id, transaction_date, amount, product_category))\n",
        "\n",
        "    return data\n",
        "\n",
        "# Generate 10000 rows of transaction data\n",
        "data = generate_data(10_000)\n",
        "\n",
        "# Convert to a Pandas DataFrame\n",
        "columns = ['CustomerID', 'TransactionID', 'TransactionDate', 'Amount', 'ProductCategory']\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Create the CSV file\n",
        "csv_path = \"sample_transactions.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"Sample CSV file '{csv_path}' generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRiKSPCyKaah",
        "outputId": "0aa43d92-c219-4647-94c3-767f49bf9333"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample CSV file 'sample_transactions.csv' generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Read the CSV File into a PySpark DataFrame"
      ],
      "metadata": {
        "id": "g4PT5XIiLw8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the first 5 rows\n",
        "spark_df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kGSq4k8KeZL",
        "outputId": "7b5677c9-629b-4e3e-dab7-fced000ca15e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+---------------+------+---------------+\n",
            "|CustomerID|TransactionID|TransactionDate|Amount|ProductCategory|\n",
            "+----------+-------------+---------------+------+---------------+\n",
            "|    C00006|       T58996|     2023-01-09| 17.02|      Furniture|\n",
            "|    C00076|       T30519|     2023-02-28|459.67|          Books|\n",
            "|    C00076|       T89246|     2023-06-10|404.95|       Clothing|\n",
            "|    C00049|       T11436|     2023-06-05| 103.9|          Books|\n",
            "|    C00049|       T18176|     2023-04-03|406.55|      Furniture|\n",
            "+----------+-------------+---------------+------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Exploring the DataFrame"
      ],
      "metadata": {
        "id": "_RIITip_L5GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the schema of the DataFrame\n",
        "spark_df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0xFp27-KgXs",
        "outputId": "a06af877-c1a1-418b-a0d0-4be8a01f3377"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CustomerID: string (nullable = true)\n",
            " |-- TransactionID: string (nullable = true)\n",
            " |-- TransactionDate: date (nullable = true)\n",
            " |-- Amount: double (nullable = true)\n",
            " |-- ProductCategory: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Filter transactions with an Amount greater than 100\n",
        "filtered_df = spark_df.filter(col(\"Amount\") > 100)\n",
        "\n",
        "# Select specific columns\n",
        "selected_df = filtered_df.select(\"CustomerID\", \"TransactionID\", \"Amount\")\n",
        "\n",
        "# Show the results\n",
        "selected_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua9R3BnoKiiQ",
        "outputId": "e19be35b-238c-4f8b-de13-e7a7efcd5afc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+------+\n",
            "|CustomerID|TransactionID|Amount|\n",
            "+----------+-------------+------+\n",
            "|    C00076|       T30519|459.67|\n",
            "|    C00076|       T89246|404.95|\n",
            "|    C00049|       T11436| 103.9|\n",
            "|    C00049|       T18176|406.55|\n",
            "|    C00096|       T31087|349.47|\n",
            "+----------+-------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}