{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing NLTK"
      ],
      "metadata": {
        "id": "2ik7yq56NsrV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csTUtV_hIudG",
        "outputId": "eabe6b0a-f4e8-4841-f869-67facc108602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download essential datasets and models\n",
        "nltk.download('punkt')  # Tokenizers for sentence and word tokenization\n",
        "nltk.download('stopwords')  # List of common stop words\n",
        "nltk.download('wordnet')  # WordNet lexical database for lemmatization\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # Part-of-speech tagger\n",
        "nltk.download('maxent_ne_chunker_tab')  # Named Entity Recognition model\n",
        "nltk.download('words')  # Word corpus for NER\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khrbb-C8J5Ip",
        "outputId": "cd75c3cf-bf1a-4ac0-bcc3-7ce7eb6bf4fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing"
      ],
      "metadata": {
        "id": "lIOaDokONw1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is cool! Let's explore it.\"\n",
        "\n",
        "# Remove punctuation using string.punctuation\n",
        "cleaned_text = ''.join(char for char in text if char not in string.punctuation)\n",
        "print(\"Text without punctuation:\", cleaned_text)\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(cleaned_text)\n",
        "print(\"Sentences:\", sentences)\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(cleaned_text)\n",
        "print(\"Words:\", words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuVHrIa7J_9U",
        "outputId": "7fecd594-1115-4b68-f2f8-ea48bfb800ae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without punctuation: Natural Language Processing NLP is cool Lets explore it\n",
            "Sentences: ['Natural Language Processing NLP is cool Lets explore it']\n",
            "Words: ['Natural', 'Language', 'Processing', 'NLP', 'is', 'cool', 'Lets', 'explore', 'it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load NLTK's stopwords list\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out stop words\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"Filtered Words:\", filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX0bZ2y2KDaL",
        "outputId": "d1576147-171d-44ab-862a-71f3383f761c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['Natural', 'Language', 'Processing', 'NLP', 'cool', 'Lets', 'explore']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Apply stemming to filtered words\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(\"Stemmed Words:\", stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vEw6L9TKFjk",
        "outputId": "38084b66-0d8e-4463-a35a-d8f454491250"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['natur', 'languag', 'process', 'nlp', 'cool', 'let', 'explor']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "yIKdaABAN2fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize each word\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in filtered_words]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZMwY0G0KIkT",
        "outputId": "ffbad6dd-64f7-40a6-e255-e5234ea43797"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['Natural', 'Language', 'Processing', 'NLP', 'cool', 'Lets', 'explore']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-of-Speech (POS) Tagging"
      ],
      "metadata": {
        "id": "zNeQphPZN56x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "# Tokenize the text into words\n",
        "text = \"She enjoys playing soccer on weekends.\"\n",
        "\n",
        "# Tokenization (words)\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# POS tagging\n",
        "tagged_words = pos_tag(words)\n",
        "print(\"Tagged Words:\", tagged_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdCZYFc9KK1j",
        "outputId": "3b345803-2707-4298-ba49-9560f50e87c4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged Words: [('She', 'PRP'), ('enjoys', 'VBZ'), ('playing', 'VBG'), ('soccer', 'NN'), ('on', 'IN'), ('weekends', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "W52aYopXOW7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"We shall visit the Eiffel Tower on our vacation to Paris.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Part-of-speech tagging\n",
        "tagged_words = pos_tag(words)\n",
        "\n",
        "# Named Entity Recognition\n",
        "named_entities = ne_chunk(tagged_words)\n",
        "print(\"Named Entities:\", named_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmPqEMqJKQqb",
        "outputId": "c3e63838-2539-4eb9-b071-6055acc40153"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: (S\n",
            "  We/PRP\n",
            "  shall/MD\n",
            "  visit/VB\n",
            "  the/DT\n",
            "  (ORGANIZATION Eiffel/NNP Tower/NNP)\n",
            "  on/IN\n",
            "  our/PRP$\n",
            "  vacation/NN\n",
            "  to/TO\n",
            "  (GPE Paris/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}